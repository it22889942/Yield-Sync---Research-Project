{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ ML Recommendation Engine â€” Google Colab Notebook\n",
    "\n",
    "This notebook trains *Labour* and *Equipment* recommendation models, computes semantic embeddings, and saves a single deployable model bundle (`.joblib`).\n",
    "\n",
    "Follow the notebook cell-by-cell. Each code cell is preceded by a Markdown explanation of what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 1 â€” Install Dependencies\n",
    "\n",
    "Installs required packages. Run this cell once at the start of the notebook. `sentence-transformers` is used for semantic embeddings; `xgboost` and `lightgbm` are optional model backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet sentence-transformers\n",
    "!pip install --quiet xgboost lightgbm joblib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Step 2 â€” Imports, Settings & Folder Setup\n",
    "\n",
    "Imports libraries, sets plotting style, and creates local folders the notebook will use (`models`, `outputs/plots`, `embeddings`, `data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs/plots', exist_ok=True)\n",
    "os.makedirs('embeddings', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print('Folders ready: models/, outputs/plots/, embeddings/, data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 3 â€” Upload your datasets (Colab)\n",
    "\n",
    "Use this cell to upload the two Excel files from your local machine:\n",
    "- `Labour_Expanded_clean_generated.xlsx`\n",
    "- `Equipment_Final_clean_generated.xlsx`\n",
    "\n",
    "If you already have these files in Google Drive, you can mount Drive instead (alternative code provided below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print('Select and upload your dataset files (Excel).')\n",
    "uploaded = files.upload()\n",
    "for name, data in uploaded.items():\n",
    "    path = os.path.join('data', name)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(data)\n",
    "    print('Saved:', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Mount Google Drive\n",
    "If your files are in Google Drive, run this cell instead of uploading manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# then set paths like:\n",
    "# LABOUR_DATA_PATH = '/content/drive/MyDrive/path/to/Labour_Expanded_clean_generated.xlsx'\n",
    "# EQUIP_DATA_PATH  = '/content/drive/MyDrive/path/to/Equipment_Final_clean_generated.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 4 â€” Load datasets\n",
    "\n",
    "This cell reads the uploaded Excel files, normalizes column names, and ensures required columns exist. It will create placeholder `unknown` values for missing columns so the rest of the pipeline won't fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABOUR_DATA_PATH = 'data/Labour_Expanded_clean_generated.xlsx'\n",
    "EQUIP_DATA_PATH  = 'data/Equipment_Final_clean_generated.xlsx'\n",
    "\n",
    "print('Reading:', LABOUR_DATA_PATH)\n",
    "DF_labour = pd.read_excel(LABOUR_DATA_PATH)\n",
    "DF_labour.columns = [c.strip() for c in DF_labour.columns]\n",
    "\n",
    "print('Reading:', EQUIP_DATA_PATH)\n",
    "DF_equip = pd.read_excel(EQUIP_DATA_PATH)\n",
    "DF_equip.columns = [c.strip() for c in DF_equip.columns]\n",
    "\n",
    "# Ensure essential columns exist and fill missing values\n",
    "for col in ['Labour_Type', 'Location', 'Season', 'Crop_Type', 'Hourly_Rate', 'Rating', 'Skill_Level', 'Name']:\n",
    "    if col not in DF_labour.columns:\n",
    "        DF_labour[col] = 'unknown'\n",
    "    DF_labour[col] = DF_labour[col].fillna('unknown')\n",
    "\n",
    "for col in ['Equipment_Type', 'For_Crop', 'Season', 'Nearest_Major_District', 'Condition']:\n",
    "    if col not in DF_equip.columns:\n",
    "        DF_equip[col] = 'unknown'\n",
    "    DF_equip[col] = DF_equip[col].fillna('unknown')\n",
    "\n",
    "print('Labour shape:', DF_labour.shape)\n",
    "print('Equipment shape:', DF_equip.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Step 5 â€” Feature engineering (text fields & numeric cleaning)\n",
    "\n",
    "Create `request_text` for both datasets by concatenating important fields. Convert numeric fields to numbers safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare labour fields\n",
    "for col in ['Labour_Type', 'Season', 'Crop_Type', 'Skill_Level', 'Location']:\n",
    "    DF_labour[col] = DF_labour[col].astype(str).str.strip()\n",
    "\n",
    "DF_labour['Hourly_Rate'] = pd.to_numeric(DF_labour['Hourly_Rate'], errors='coerce').fillna(0)\n",
    "DF_labour['Rating'] = pd.to_numeric(DF_labour['Rating'], errors='coerce').fillna(0)\n",
    "\n",
    "# Clean equipment\n",
    "for col in ['Equipment_Type', 'For_Crop', 'Season', 'Nearest_Major_District', 'Condition']:\n",
    "    DF_equip[col] = DF_equip[col].astype(str).str.strip()\n",
    "\n",
    "if 'Hourly_Rate_LKR' in DF_equip.columns:\n",
    "    DF_equip['Hourly_Rate_LKR'] = pd.to_numeric(DF_equip['Hourly_Rate_LKR'], errors='coerce').fillna(0)\n",
    "if 'Rating' in DF_equip.columns:\n",
    "    DF_equip['Rating'] = pd.to_numeric(DF_equip['Rating'], errors='coerce').fillna(0)\n",
    "\n",
    "# Build request_text fields (what a user might type)\n",
    "DF_labour['request_text'] = (\n",
    "    DF_labour['Labour_Type'] + ' ' +\n",
    "    DF_labour['Season'] + ' ' +\n",
    "    DF_labour['Crop_Type'] + ' ' +\n",
    "    DF_labour['Skill_Level'] + ' ' +\n",
    "    DF_labour['Location']\n",
    ")\n",
    "\n",
    "DF_equip['request_text'] = (\n",
    "    DF_equip['Equipment_Type'] + ' ' +\n",
    "    DF_equip['For_Crop'] + ' ' +\n",
    "    DF_equip['Season'] + ' ' +\n",
    "    DF_equip['Nearest_Major_District'] + ' ' +\n",
    "    DF_equip['Condition']\n",
    ")\n",
    "\n",
    "# Collapse rare labour types (optional)\n",
    "vc = DF_labour['Labour_Type'].value_counts()\n",
    "rare = vc[vc < 5].index.tolist()\n",
    "DF_labour['Labour_Type_collapsed'] = DF_labour['Labour_Type'].apply(\n",
    "    lambda x: x if x not in rare else 'other'\n",
    ")\n",
    "\n",
    "labour_label_col = 'Labour_Type_collapsed'\n",
    "equipment_label_col = 'Equipment_Type'\n",
    "\n",
    "print('Feature engineering complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Step 6 â€” Fit a shared TF-IDF vectorizer\n",
    "\n",
    "We fit one vectorizer on the combined text of both datasets so that the vocabulary is shared across labour & equipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([\n",
    "    DF_labour['request_text'],\n",
    "    DF_equip['request_text']\n",
    "], ignore_index=True).values\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=None,\n",
    "    ngram_range=(1, 3),\n",
    "    max_df=0.98,\n",
    "    min_df=1,\n",
    "    max_features=60000\n",
    ")\n",
    "print('Fitting TF-IDF on combined text...')\n",
    "vectorizer.fit(all_text)\n",
    "print('Vocabulary size:', len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 7 â€” Prepare Labour data and compare models (5-fold CV)\n",
    "\n",
    "We compare RandomForest, XGBoost and LightGBM using stratified 5-fold CV on the labour training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labour_text = DF_labour['request_text'].values\n",
    "y_labour = DF_labour[labour_label_col].values\n",
    "\n",
    "le_labour = LabelEncoder()\n",
    "y_labour_enc = le_labour.fit_transform(y_labour)\n",
    "\n",
    "X_lab_train_text, X_lab_test_text, y_lab_train, y_lab_test = train_test_split(\n",
    "    X_labour_text, y_labour_enc, test_size=0.2, random_state=42, stratify=y_labour_enc\n",
    ")\n",
    "\n",
    "X_lab_train = vectorizer.transform(X_lab_train_text)\n",
    "X_lab_test  = vectorizer.transform(X_lab_test_text)\n",
    "\n",
    "print('Labour - Train samples:', X_lab_train.shape[0], 'Test samples:', X_lab_test.shape[0])\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=400, max_depth=6, learning_rate=0.1, eval_metric='mlogloss', n_jobs=-1, random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=400, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print('Evaluating', name)\n",
    "    scores = cross_val_score(clf, X_lab_train, y_lab_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "    results.append({'model': name, 'mean_acc': scores.mean(), 'std_acc': scores.std()})\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values('mean_acc', ascending=False)\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† Step 8 â€” Train the best Labour model and evaluate on test set\n",
    "\n",
    "We retrain the best model on the training split and evaluate on the hold-out test split. The model, vectorizer and label encoder are saved to `models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name_labour = res_df.iloc[0]['model']\n",
    "best_labour_clf = models[best_name_labour]\n",
    "print('Training best model:', best_name_labour)\n",
    "best_labour_clf.fit(X_lab_train, y_lab_train)\n",
    "\n",
    "y_lab_pred = best_labour_clf.predict(X_lab_test)\n",
    "print('Labour Test Accuracy:', round(accuracy_score(y_lab_test, y_lab_pred), 4))\n",
    "print(classification_report(y_lab_test, y_lab_pred, target_names=le_labour.classes_))\n",
    "\n",
    "cm_lab = confusion_matrix(y_lab_test, y_lab_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_lab, annot=False, cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - Labour ({best_name_labour})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('outputs/plots/confusion_matrix_labour.png')\n",
    "plt.close()\n",
    "\n",
    "# Save labour-only artifact\n",
    "joblib.dump({'vectorizer': vectorizer, 'label_encoder_labour': le_labour, 'labour_model': best_labour_clf},\n",
    "            f'models/best_labour_model_{best_name_labour}.joblib')\n",
    "print('Saved labour model artifact')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 9 â€” Prepare Equipment data and train equipment model\n",
    "\n",
    "We reuse the **same vectorizer** and the same *type* of model as the winner for labour for simplicity. This keeps model configuration consistent across both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_equip_text = DF_equip['request_text'].values\n",
    "y_equip = DF_equip[equipment_label_col].values\n",
    "\n",
    "le_equip = LabelEncoder()\n",
    "y_equip_enc = le_equip.fit_transform(y_equip)\n",
    "\n",
    "X_equip_train_text, X_equip_test_text, y_equip_train, y_equip_test = train_test_split(\n",
    "    X_equip_text, y_equip_enc, test_size=0.2, random_state=42, stratify=y_equip_enc\n",
    ")\n",
    "\n",
    "X_equip_train = vectorizer.transform(X_equip_train_text)\n",
    "X_equip_test  = vectorizer.transform(X_equip_test_text)\n",
    "\n",
    "print('Equipment - Train samples:', X_equip_train.shape[0], 'Test samples:', X_equip_test.shape[0])\n",
    "\n",
    "# Instantiate same model class as best_labour_clf but with fresh parameters\n",
    "if best_name_labour == 'RandomForest':\n",
    "    equip_clf = RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "elif best_name_labour == 'XGBoost':\n",
    "    equip_clf = xgb.XGBClassifier(n_estimators=400, max_depth=6, learning_rate=0.1, eval_metric='mlogloss', n_jobs=-1, random_state=42)\n",
    "else:\n",
    "    equip_clf = lgb.LGBMClassifier(n_estimators=400, learning_rate=0.1, random_state=42)\n",
    "\n",
    "print('Training equipment model...')\n",
    "equip_clf.fit(X_equip_train, y_equip_train)\n",
    "\n",
    "y_equip_pred = equip_clf.predict(X_equip_test)\n",
    "print('Equipment Test Accuracy:', round(accuracy_score(y_equip_test, y_equip_pred), 4))\n",
    "print(classification_report(y_equip_test, y_equip_pred, target_names=le_equip.classes_))\n",
    "\n",
    "cm_equip = confusion_matrix(y_equip_test, y_equip_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_equip, annot=False, cmap='Greens')\n",
    "plt.title(f'Confusion Matrix - Equipment ({best_name_labour}-based)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('outputs/plots/confusion_matrix_equipment.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 10 â€” Semantic Embeddings (SentenceTransformer)\n",
    "\n",
    "Compute embeddings for both datasets using `all-MiniLM-L6-v2`. These embeddings are useful for similarity search and ranking.\n",
    "We save embeddings as numpy arrays in the saved DataFrame to keep the saved joblib file lightweight and portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Computing semantic embeddings (labour + equipment)...\")\n",
    "model_semantic = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create text used for embedding (optionally include label to give the embedding extra context)\n",
    "DF_labour['text_for_embed'] = DF_labour['request_text'] + ' ' + DF_labour['Labour_Type'].astype(str)\n",
    "DF_equip['text_for_embed'] = (\n",
    "    DF_equip['Equipment_Type'].astype(str) + ' ' +\n",
    "    DF_equip['For_Crop'].astype(str) + ' ' +\n",
    "    DF_equip['Season'].astype(str) + ' ' +\n",
    "    DF_equip['Nearest_Major_District'].astype(str)\n",
    ")\n",
    "\n",
    "print('Computing labour embeddings...')\n",
    "DF_labour['embedding'] = DF_labour['text_for_embed'].apply(lambda x: model_semantic.encode(x))\n",
    "print('Computing equipment embeddings...')\n",
    "DF_equip['embedding'] = DF_equip['text_for_embed'].apply(lambda x: model_semantic.encode(x))\n",
    "\n",
    "# Convert embeddings to numpy arrays (they already are numpy arrays from encode)\n",
    "DF_labour_save = DF_labour.copy()\n",
    "DF_equip_save = DF_equip.copy()\n",
    "\n",
    "print('Embeddings computed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 11 â€” Save complete model bundle (`joblib`)\n",
    "\n",
    "We bundle classifiers, vectorizer, label encoders and datasets (with embeddings) into one file: `models/complete_recommendation_model.joblib`.\n",
    "This file can be loaded later with `joblib.load()` for inference or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\n",
    "    'classifier_model_labour': best_labour_clf,\n",
    "    'classifier_model_equipment': equip_clf,\n",
    "    'vectorizer': vectorizer,\n",
    "    'label_encoder_labour': le_labour,\n",
    "    'label_encoder_equipment': le_equip,\n",
    "    'DF_labour': DF_labour_save,\n",
    "    'DF_equipment': DF_equip_save,\n",
    "    'embedding_model_name': 'all-MiniLM-L6-v2'\n",
    "}\n",
    "\n",
    "joblib.dump(all_models, 'models/complete_recommendation_model.joblib')\n",
    "print('Saved models/complete_recommendation_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Done â€” Next steps / Tips\n",
    "\n",
    "- Download the `models/complete_recommendation_model.joblib` file and move it to your server for production use.\n",
    "- Consider using FAISS or Annoy for fast nearest-neighbour search on embeddings.\n",
    "- For production inference, use the same TF-IDF vectorizer and label encoders saved in the joblib bundle.\n",
    "- If you want, I can add a prediction UI (Gradio) cell to this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
